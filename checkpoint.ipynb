{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import LabelAccuracyEvaluator\n",
    "\n",
    "from sentence_transformers import SentenceTransformer,  InputExample, losses, models, util\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import math\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import  re\n",
    "import  yaml\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/all-MiniLM-L12-v2'\n",
    "\n",
    "\n",
    "\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n",
    "\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "logging.info(\"Read STSbenchmark train dataset\")\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_batch_size = 32\n",
    "\n",
    "num_epochs = 6\n",
    "\n",
    "distance_limit = 10\n",
    "csv_pair_size_limit = 100\n",
    "split_method_index = 1\n",
    "# CUDA_num = int(input(\"please input CUDA number \"))\n",
    "# device = torch.device(f\"cuda:{CUDA_num}\")\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "negative_sample_num = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "debates_path = '/Users/fanzhe/Desktop/master_thesis/Data/kialo_debatetree_data/csv_sample_nofilter'\n",
    "model_save_path = '/Users/fanzhe/Desktop/master_thesis/Data/model_ouput/training_stsbenchmark_'+model_name.replace(\"/\", \"-\")+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# 获取文件夹中的所有文件\n",
    "all_files = os.listdir(debates_path)\n",
    "\n",
    "# 筛选出CSV文件\n",
    "csv_files = [file for file in all_files if file.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_argument(text):\n",
    "    # 定义正则表达式\n",
    "    pattern = r\"-> See \\d+(\\.\\d+)+\\.\"\n",
    "\n",
    "    # 使用re.match进行匹配\n",
    "    return bool(re.match(pattern, text))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_split_method_1(max_pairs_size, max_distance):\n",
    "    \n",
    "\n",
    "    all_files = os.listdir(debates_path)\n",
    "    # csv_files = [file for file in all_files if file.endswith('.csv') and  (len(pd.read_csv(os.path.join(debates_path, file)))) < max_pairs_size  ]\n",
    "    less_than_limit_files = []\n",
    "    over_limit_files = []\n",
    "    for file in all_files:\n",
    "        if file.endswith('.csv'):\n",
    "            if (len(pd.read_csv(os.path.join(debates_path, file)))) < max_pairs_size:\n",
    "                less_than_limit_files.append(file)\n",
    "            elif (len(pd.read_csv(os.path.join(debates_path, file)))) >= max_pairs_size:\n",
    "                over_limit_files.append(file)\n",
    "    random.shuffle(over_limit_files)\n",
    "    random.shuffle(less_than_limit_files)\n",
    "\n",
    "    # shuffled_csv_files = csv_files\n",
    "    samples = []\n",
    "\n",
    "    # 逐个读取CSV文件\n",
    "    content_1_list = []\n",
    "\n",
    "    for file in less_than_limit_files:\n",
    "\n",
    "        file_path = os.path.join(debates_path, file)\n",
    "        # 读取CSV文件\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        number_of_pairs = len(df)\n",
    "\n",
    "        # 按行处理数据\n",
    "        files_has_0_distance = []\n",
    "        for index, row in df.iterrows():\n",
    "            if float(row['distance']) != 0 and not skip_argument(row['content_1']) and not skip_argument(row['content_2']) and float(row['distance']) <= max_distance:\n",
    "\n",
    "                if float(row['polarity_consistency']) == 1:\n",
    "\n",
    "                    score = 1 # Normalize score to range 0 ... 1\n",
    "                    inp_example = InputExample(texts=[row['content_1'], row['content_2']], label=score)\n",
    "                    samples.append(inp_example)\n",
    "\n",
    "                if float(row['polarity_consistency']) == -1:\n",
    "                    if float(row['polarity_1']) == 0:\n",
    "                        # score = 1\n",
    "                        # inp_example = InputExample(texts=[row['content_1'], row['content_2']], label=score)\n",
    "                        pass\n",
    "\n",
    "                    elif float(row['polarity_1']) != 0:\n",
    "                        score = 0\n",
    "                        inp_example = InputExample(texts=[row['content_1'], row['content_2']], label=score)\n",
    "\n",
    "                        samples.append(inp_example)\n",
    "\n",
    "            elif float(row['distance']) == 0:\n",
    "                files_has_0_distance.append(file_path)\n",
    "\n",
    "    for file in over_limit_files:\n",
    "\n",
    "        file_path = os.path.join(debates_path, file)\n",
    "        # 读取CSV文件\n",
    "        df = pd.read_csv(file_path)\n",
    "        sampled_df = df.sample(n=max_pairs_size)\n",
    "\n",
    "        number_of_pairs = len(sampled_df)\n",
    "\n",
    "\n",
    "        # 按行处理数据\n",
    "        files_has_0_distance = []\n",
    "        for index, row in sampled_df.iterrows():\n",
    "            if float(row['distance']) != 0 and not skip_argument(row['content_1']) and not skip_argument(\n",
    "                    row['content_2']) and float(row['distance']) <= max_distance:\n",
    "\n",
    "                if float(row['polarity_consistency']) == 1:\n",
    "                    score = 1  # Normalize score to range 0 ... 1\n",
    "                    inp_example = InputExample(texts=[row['content_1'], row['content_2']], label=score)\n",
    "                    # print(score, row['content_1'], row['content_2'])\n",
    "                    samples.append(inp_example)\n",
    "\n",
    "                if float(row['polarity_consistency']) == -1:\n",
    "                    if float(row['polarity_1']) == 0:\n",
    "                        # score = 1\n",
    "                        # inp_example = InputExample(texts=[row['content_1'], row['content_2']], label=score)\n",
    "                        pass\n",
    "\n",
    "                    elif float(row['polarity_1']) != 0:\n",
    "                        score = 0\n",
    "                        inp_example = InputExample(texts=[row['content_1'], row['content_2']], label=score)\n",
    "\n",
    "                        samples.append(inp_example)\n",
    "\n",
    "            elif float(row['distance']) == 0:\n",
    "                files_has_0_distance.append(file_path)\n",
    "\n",
    "\n",
    "    file_name = \"files_has_0_distance.txt\"\n",
    "\n",
    "    # 使用 'with' 语句打开文件进行写入，确保文件最后会被正确关闭\n",
    "    with open(file_name, \"w\") as file:\n",
    "        # 遍历列表，写入每一行\n",
    "        for line in files_has_0_distance:\n",
    "            file.write(line + \"\\n\")  # \"\\n\" 是换行符\n",
    "\n",
    "    # print(samples, type(samples))\n",
    "\n",
    "    random.shuffle(samples)\n",
    "    shuffled_samples = samples\n",
    "    sample_collection = shuffled_samples\n",
    "   \n",
    "    return sample_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relatedness_split_method_1(max_pairs_size, max_distance):\n",
    "\n",
    "\n",
    "    all_files = os.listdir(debates_path)\n",
    "    # csv_files = [file for file in all_files if file.endswith('.csv') and  (len(pd.read_csv(os.path.join(debates_path, file)))) < max_pairs_size  ]\n",
    "    less_than_limit_files = []\n",
    "    over_limit_files = []\n",
    "    for file in all_files:\n",
    "        if file.endswith('.csv'):\n",
    "            if (len(pd.read_csv(os.path.join(debates_path, file)))) < max_pairs_size:\n",
    "                less_than_limit_files.append(file)\n",
    "            elif (len(pd.read_csv(os.path.join(debates_path, file)))) >= max_pairs_size:\n",
    "                over_limit_files.append(file)\n",
    "    random.shuffle(over_limit_files)\n",
    "    random.shuffle(less_than_limit_files)\n",
    "\n",
    "    # shuffled_csv_files = csv_files\n",
    "    samples = []\n",
    "\n",
    "    # 逐个读取CSV文件\n",
    "    content_1_list = []\n",
    "\n",
    "    for file in less_than_limit_files:\n",
    "\n",
    "        file_path = os.path.join(debates_path, file)\n",
    "        # 读取CSV文件\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        number_of_pairs = len(df)\n",
    "\n",
    "        # 按行处理数据\n",
    "        files_has_0_distance = []\n",
    "        for index, row in df.iterrows():\n",
    "            if float(row['distance']) != 0 and not skip_argument(row['content_1']) and not skip_argument(row['content_2']) and float(row['distance']) <= max_distance:\n",
    "                score =  1/ float(row['distance']) # Normalize score to range 0 ... 1\n",
    "                if type(score) is not  float:\n",
    "                    print(\"scoretype\", type(score))\n",
    "                inp_example = InputExample(texts=[row['content_1'], row['content_2']], label=score)\n",
    "                # print(score, row['content_1'], row['content_2'])\n",
    "\n",
    "                samples.append(inp_example)\n",
    "                file_index, extension = os.path.splitext(file)\n",
    "                if row['content_1'] not in content_1_list:\n",
    "                    content_1_list.append({\"file_index\": str(file_index), \"content\": row['content_1']})\n",
    "\n",
    "            elif float(row['distance']) == 0:\n",
    "                files_has_0_distance.append(file_path)\n",
    "\n",
    "                print(file_path, row['distance'])\n",
    "    for file in over_limit_files:\n",
    "\n",
    "        file_path = os.path.join(debates_path, file)\n",
    "        # 读取CSV文件\n",
    "        df = pd.read_csv(file_path)\n",
    "        sampled_df = df.sample(n=max_pairs_size)\n",
    "\n",
    "        number_of_pairs = len(sampled_df)\n",
    "\n",
    "\n",
    "        # 按行处理数据\n",
    "        files_has_0_distance = []\n",
    "        for index, row in sampled_df.iterrows():\n",
    "            if float(row['distance']) != 0 and not skip_argument(row['content_1']) and not skip_argument(row['content_2']) and float(row['distance']) <= max_distance:\n",
    "                score =  1/ float(row['distance']) # Normalize score to range 0 ... 1\n",
    "                if type(score) is not  float:\n",
    "                    print(\"scoretype\", type(score))\n",
    "                inp_example = InputExample(texts=[row['content_1'], row['content_2']], label=score)\n",
    "                # print(score, row['content_1'], row['content_2'])\n",
    "\n",
    "                samples.append(inp_example)\n",
    "                file_index, extension = os.path.splitext(file)\n",
    "                if row['content_1'] not in content_1_list:\n",
    "                    content_1_list.append({\"file_index\": str(file_index), \"content\": row['content_1']})\n",
    "\n",
    "            elif float(row['distance']) == 0:\n",
    "                files_has_0_distance.append(file_path)\n",
    "\n",
    "                print(file_path, row['distance'])\n",
    "#     for content_1 in content_1_list:\n",
    "#         # print(\"testhahaha\", content_1,type(content_1))\n",
    "#         rest_of_contents = [f for f in content_1_list if f[\"file_index\"] != content_1[\"file_index\"]]\n",
    "#         random_negative_arguments = []\n",
    "#         while len(random_negative_arguments) < negative_sample_num:\n",
    "#             random_index_content = random.choice(rest_of_contents)\n",
    "#             random_content = random_index_content[\"content\"]\n",
    "#             # print(random_content)\n",
    "#             if random_content not in random_negative_arguments:\n",
    "#                 random_negative_arguments.append(random_content)\n",
    "\n",
    "#         for negative_argument in random_negative_arguments:\n",
    "#             # print(\"test\", content_1, negative_argument, type(negative_argument))\n",
    "#             neg_inp_example = InputExample(texts=[content_1[\"content\"], negative_argument], label=0.0)\n",
    "#             samples.append(neg_inp_example)\n",
    "#     print(\"shuffle seed test, negative\", random_negative_arguments[:10])\n",
    "\n",
    "    file_name = \"files_has_0_distance.txt\"\n",
    "\n",
    "    # 使用 'with' 语句打开文件进行写入，确保文件最后会被正确关闭\n",
    "    with open(file_name, \"w\") as file:\n",
    "        # 遍历列表，写入每一行\n",
    "        for line in files_has_0_distance:\n",
    "            file.write(line + \"\\n\")  # \"\\n\" 是换行符\n",
    "\n",
    "    # print(samples, type(samples))\n",
    "\n",
    "    random.shuffle(samples)\n",
    "    shuffled_samples = samples\n",
    "    sample_collection = shuffled_samples\n",
    "\n",
    "    return sample_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content 1: From an artistic point of view I think it's not, just classic gaming lovers interacting with each other, so gamers are just enjoying the kind of games they love as much as they can.\n",
      "Content 2: There is only so much you can do with pixel graphics. The art-style is limiting and stops innovation.\n",
      "cosine_similarity:0.3311109244823456\n",
      "Polarity Label: 0\n",
      "------------------------------\n",
      "Content 1: He fought bravely, according to Aragon.\n",
      "Content 2: The One Ring is specifically designed to corrupt and bend the minds of Men, Dwarves, and Elves. No man can stem off its corruption forever, and that was the same of the Hobbits \\(albeit much slower\\).\n",
      "cosine_similarity:0.14857426285743713\n",
      "Polarity Label: 0\n",
      "------------------------------\n",
      "Content 1: In the vast majority of cases, slamming on the brakes has the best chance of [minimising the total amount of harm caused](https://www.theguardian.com/technology/2016/aug/22/self-driving-cars-moral-dilemmas), as it is the most controllable action. Programming self-driving cars to prioritise their passengers would often stop them taking this course of action, which would actually increase the overall harm caused.\n",
      "Content 2: It is much more likely that the advent of self-driving cars will [move liability](http://www.huffingtonpost.com/allan-smith/how-selfdriving-cars-will_b_10890256.html) from the passengers to car manufacturers.\n",
      "cosine_similarity:0.49887266755104065\n",
      "Polarity Label: 1\n",
      "------------------------------\n",
      "Content 1: A forensic linguistic analysis conducted by the [BBC](https://www.bbc.com/news/world-us-canada-45435813) concluded that Pence was the most likely author.\n",
      "Content 2: Someone could have observed and imitated Pence's writing style to attract attention to Pence and away from the real author.\n",
      "cosine_similarity:0.6918931007385254\n",
      "Relatedness Label: 1.0\n",
      "------------------------------\n",
      "Content 1: Canonically, Goku has died three times, and never won the World Martial Arts Tournament, thus proving that even within his own pond he is neither strongest nor undefeatable.\n",
      "Content 2: The Hulk is the most powerful fantasy hero and would win in a last entity standing style free for all.\n",
      "cosine_similarity:0.38283106684684753\n",
      "Relatedness Label: 0.3333333333333333\n",
      "------------------------------\n",
      "Content 1: The efforts of going to school \\(such as getting ready and travel to the school\\) take away from learning. If a student just puts on VR goggles, that would be much less distracting than going to a brick-and-mortar school.\n",
      "Content 2: VR provides a safe zone for students.\n",
      "cosine_similarity:0.6197353601455688\n",
      "Relatedness Label: 0.3333333333333333\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "polarity_sample_collection = polarity_split_method_1(csv_pair_size_limit, distance_limit)\n",
    "relatedness_sample_collection = relatedness_split_method_1(csv_pair_size_limit, distance_limit)\n",
    "polarity_selected_samples = random.sample(polarity_sample_collection, 3)\n",
    "relatedness_selected_samples = random.sample(relatedness_sample_collection, 3)\n",
    "\n",
    "\n",
    "for idx, sample in enumerate(polarity_selected_samples, 1):\n",
    "    sentence1, sentence2 = sample.texts[0], sample.texts[1]\n",
    "    embeddings = model.encode([sentence1, sentence2], convert_to_tensor=True)\n",
    "\n",
    "    cosine_similarity = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()\n",
    "#     print(f\"Sample {idx}:\")\n",
    "    print(f\"Content 1: {sample.texts[0]}\")\n",
    "    print(f\"Content 2: {sample.texts[1]}\")\n",
    "    print(f\"cosine_similarity:{cosine_similarity}\")\n",
    "    print(f\"Polarity Label: {sample.label}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "for idx, sample in enumerate(relatedness_selected_samples, 1):\n",
    "    sentence1, sentence2 = sample.texts[0], sample.texts[1]\n",
    "    embeddings = model.encode([sentence1, sentence2], convert_to_tensor=True)\n",
    "\n",
    "    cosine_similarity = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()\n",
    "#     print(f\"Sample {idx}:\")\n",
    "    print(f\"Content 1: {sample.texts[0]}\")\n",
    "    print(f\"Content 2: {sample.texts[1]}\")\n",
    "    print(f\"cosine_similarity:{cosine_similarity}\")\n",
    "    print(f\"Relatedness Label: {sample.label}\")\n",
    "    print(\"-\" * 30)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
