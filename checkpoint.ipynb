{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import LabelAccuracyEvaluator\n",
    "\n",
    "from sentence_transformers import SentenceTransformer,  InputExample, losses, models, util\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import math\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import  re\n",
    "import  yaml\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/all-MiniLM-L12-v2'\n",
    "\n",
    "\n",
    "\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n",
    "\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "logging.info(\"Read STSbenchmark train dataset\")\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_batch_size = 32\n",
    "\n",
    "num_epochs = 6\n",
    "\n",
    "distance_limit = 10\n",
    "csv_pair_size_limit = 100\n",
    "split_method_index = 1\n",
    "# CUDA_num = int(input(\"please input CUDA number \"))\n",
    "# device = torch.device(f\"cuda:{CUDA_num}\")\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "negative_sample_num = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "debates_path = '/Users/fanzhe/Desktop/master_thesis/Data/kialo_debatetree_data/csv_sample_nofilter'\n",
    "model_save_path = '/Users/fanzhe/Desktop/master_thesis/Data/model_ouput/training_stsbenchmark_'+model_name.replace(\"/\", \"-\")+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# 获取文件夹中的所有文件\n",
    "all_files = os.listdir(debates_path)\n",
    "\n",
    "# 筛选出CSV文件\n",
    "csv_files = [file for file in all_files if file.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_argument(text):\n",
    "    # 定义正则表达式\n",
    "    pattern = r\"-> See \\d+(\\.\\d+)+\\.\"\n",
    "\n",
    "    # 使用re.match进行匹配\n",
    "    return bool(re.match(pattern, text))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_split_method_1(max_pairs_size, max_distance):\n",
    "    \n",
    "\n",
    "    all_files = os.listdir(debates_path)\n",
    "    # csv_files = [file for file in all_files if file.endswith('.csv') and  (len(pd.read_csv(os.path.join(debates_path, file)))) < max_pairs_size  ]\n",
    "    less_than_limit_files = []\n",
    "    over_limit_files = []\n",
    "    for file in all_files:\n",
    "        if file.endswith('.csv'):\n",
    "            if (len(pd.read_csv(os.path.join(debates_path, file)))) < max_pairs_size:\n",
    "                less_than_limit_files.append(file)\n",
    "            elif (len(pd.read_csv(os.path.join(debates_path, file)))) >= max_pairs_size:\n",
    "                over_limit_files.append(file)\n",
    "    random.shuffle(over_limit_files)\n",
    "    random.shuffle(less_than_limit_files)\n",
    "\n",
    "    # shuffled_csv_files = csv_files\n",
    "    samples = []\n",
    "\n",
    "    # 逐个读取CSV文件\n",
    "    content_1_list = []\n",
    "\n",
    "    for file in less_than_limit_files:\n",
    "\n",
    "        file_path = os.path.join(debates_path, file)\n",
    "        # 读取CSV文件\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        number_of_pairs = len(df)\n",
    "\n",
    "        # 按行处理数据\n",
    "        files_has_0_distance = []\n",
    "        for index, row in df.iterrows():\n",
    "            if float(row['distance']) != 0 and not skip_argument(row['content_1']) and not skip_argument(row['content_2']) and float(row['distance']) <= max_distance:\n",
    "\n",
    "                if float(row['polarity_consistency']) == 1:\n",
    "\n",
    "                    score = 1 # Normalize score to range 0 ... 1\n",
    "                    inp_example = InputExample(texts=[row['content_1'], row['content_2']], label=score)\n",
    "                    samples.append(inp_example)\n",
    "\n",
    "                if float(row['polarity_consistency']) == -1:\n",
    "                    if float(row['polarity_1']) == 0:\n",
    "                        # score = 1\n",
    "                        # inp_example = InputExample(texts=[row['content_1'], row['content_2']], label=score)\n",
    "                        pass\n",
    "\n",
    "                    elif float(row['polarity_1']) != 0:\n",
    "                        score = 0\n",
    "                        inp_example = InputExample(texts=[row['content_1'], row['content_2']], label=score)\n",
    "\n",
    "                        samples.append(inp_example)\n",
    "\n",
    "            elif float(row['distance']) == 0:\n",
    "                files_has_0_distance.append(file_path)\n",
    "\n",
    "    for file in over_limit_files:\n",
    "\n",
    "        file_path = os.path.join(debates_path, file)\n",
    "        # 读取CSV文件\n",
    "        df = pd.read_csv(file_path)\n",
    "        sampled_df = df.sample(n=max_pairs_size)\n",
    "\n",
    "        number_of_pairs = len(sampled_df)\n",
    "\n",
    "\n",
    "        # 按行处理数据\n",
    "        files_has_0_distance = []\n",
    "        for index, row in sampled_df.iterrows():\n",
    "            if float(row['distance']) != 0 and not skip_argument(row['content_1']) and not skip_argument(\n",
    "                    row['content_2']) and float(row['distance']) <= max_distance:\n",
    "\n",
    "                if float(row['polarity_consistency']) == 1:\n",
    "                    score = 1  # Normalize score to range 0 ... 1\n",
    "                    inp_example = InputExample(texts=[row['content_1'], row['content_2']], label=score)\n",
    "                    # print(score, row['content_1'], row['content_2'])\n",
    "                    samples.append(inp_example)\n",
    "\n",
    "                if float(row['polarity_consistency']) == -1:\n",
    "                    if float(row['polarity_1']) == 0:\n",
    "                        # score = 1\n",
    "                        # inp_example = InputExample(texts=[row['content_1'], row['content_2']], label=score)\n",
    "                        pass\n",
    "\n",
    "                    elif float(row['polarity_1']) != 0:\n",
    "                        score = 0\n",
    "                        inp_example = InputExample(texts=[row['content_1'], row['content_2']], label=score)\n",
    "\n",
    "                        samples.append(inp_example)\n",
    "\n",
    "            elif float(row['distance']) == 0:\n",
    "                files_has_0_distance.append(file_path)\n",
    "\n",
    "\n",
    "    file_name = \"files_has_0_distance.txt\"\n",
    "\n",
    "    # 使用 'with' 语句打开文件进行写入，确保文件最后会被正确关闭\n",
    "    with open(file_name, \"w\") as file:\n",
    "        # 遍历列表，写入每一行\n",
    "        for line in files_has_0_distance:\n",
    "            file.write(line + \"\\n\")  # \"\\n\" 是换行符\n",
    "\n",
    "    # print(samples, type(samples))\n",
    "\n",
    "    random.shuffle(samples)\n",
    "    shuffled_samples = samples\n",
    "    sample_collection = shuffled_samples\n",
    "   \n",
    "    return sample_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relatedness_split_method_1(max_pairs_size, max_distance):\n",
    "\n",
    "\n",
    "    all_files = os.listdir(debates_path)\n",
    "    # csv_files = [file for file in all_files if file.endswith('.csv') and  (len(pd.read_csv(os.path.join(debates_path, file)))) < max_pairs_size  ]\n",
    "    less_than_limit_files = []\n",
    "    over_limit_files = []\n",
    "    for file in all_files:\n",
    "        if file.endswith('.csv'):\n",
    "            if (len(pd.read_csv(os.path.join(debates_path, file)))) < max_pairs_size:\n",
    "                less_than_limit_files.append(file)\n",
    "            elif (len(pd.read_csv(os.path.join(debates_path, file)))) >= max_pairs_size:\n",
    "                over_limit_files.append(file)\n",
    "    random.shuffle(over_limit_files)\n",
    "    random.shuffle(less_than_limit_files)\n",
    "\n",
    "    # shuffled_csv_files = csv_files\n",
    "    samples = []\n",
    "\n",
    "    # 逐个读取CSV文件\n",
    "    content_1_list = []\n",
    "\n",
    "    for file in less_than_limit_files:\n",
    "\n",
    "        file_path = os.path.join(debates_path, file)\n",
    "        # 读取CSV文件\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        number_of_pairs = len(df)\n",
    "\n",
    "        # 按行处理数据\n",
    "        files_has_0_distance = []\n",
    "        for index, row in df.iterrows():\n",
    "            if float(row['distance']) != 0 and not skip_argument(row['content_1']) and not skip_argument(row['content_2']) and float(row['distance']) <= max_distance:\n",
    "                score =  1/ float(row['distance']) # Normalize score to range 0 ... 1\n",
    "                if type(score) is not  float:\n",
    "                    print(\"scoretype\", type(score))\n",
    "                inp_example = InputExample(texts=[row['content_1'], row['content_2']], label=score)\n",
    "                # print(score, row['content_1'], row['content_2'])\n",
    "\n",
    "                samples.append(inp_example)\n",
    "                file_index, extension = os.path.splitext(file)\n",
    "                if row['content_1'] not in content_1_list:\n",
    "                    content_1_list.append({\"file_index\": str(file_index), \"content\": row['content_1']})\n",
    "\n",
    "            elif float(row['distance']) == 0:\n",
    "                files_has_0_distance.append(file_path)\n",
    "\n",
    "                print(file_path, row['distance'])\n",
    "    for file in over_limit_files:\n",
    "\n",
    "        file_path = os.path.join(debates_path, file)\n",
    "        # 读取CSV文件\n",
    "        df = pd.read_csv(file_path)\n",
    "        sampled_df = df.sample(n=max_pairs_size)\n",
    "\n",
    "        number_of_pairs = len(sampled_df)\n",
    "\n",
    "\n",
    "        # 按行处理数据\n",
    "        files_has_0_distance = []\n",
    "        for index, row in sampled_df.iterrows():\n",
    "            if float(row['distance']) != 0 and not skip_argument(row['content_1']) and not skip_argument(row['content_2']) and float(row['distance']) <= max_distance:\n",
    "                score =  1/ float(row['distance']) # Normalize score to range 0 ... 1\n",
    "                if type(score) is not  float:\n",
    "                    print(\"scoretype\", type(score))\n",
    "                inp_example = InputExample(texts=[row['content_1'], row['content_2']], label=score)\n",
    "                # print(score, row['content_1'], row['content_2'])\n",
    "\n",
    "                samples.append(inp_example)\n",
    "                file_index, extension = os.path.splitext(file)\n",
    "                if row['content_1'] not in content_1_list:\n",
    "                    content_1_list.append({\"file_index\": str(file_index), \"content\": row['content_1']})\n",
    "\n",
    "            elif float(row['distance']) == 0:\n",
    "                files_has_0_distance.append(file_path)\n",
    "\n",
    "                print(file_path, row['distance'])\n",
    "#     for content_1 in content_1_list:\n",
    "#         # print(\"testhahaha\", content_1,type(content_1))\n",
    "#         rest_of_contents = [f for f in content_1_list if f[\"file_index\"] != content_1[\"file_index\"]]\n",
    "#         random_negative_arguments = []\n",
    "#         while len(random_negative_arguments) < negative_sample_num:\n",
    "#             random_index_content = random.choice(rest_of_contents)\n",
    "#             random_content = random_index_content[\"content\"]\n",
    "#             # print(random_content)\n",
    "#             if random_content not in random_negative_arguments:\n",
    "#                 random_negative_arguments.append(random_content)\n",
    "\n",
    "#         for negative_argument in random_negative_arguments:\n",
    "#             # print(\"test\", content_1, negative_argument, type(negative_argument))\n",
    "#             neg_inp_example = InputExample(texts=[content_1[\"content\"], negative_argument], label=0.0)\n",
    "#             samples.append(neg_inp_example)\n",
    "#     print(\"shuffle seed test, negative\", random_negative_arguments[:10])\n",
    "\n",
    "    file_name = \"files_has_0_distance.txt\"\n",
    "\n",
    "    # 使用 'with' 语句打开文件进行写入，确保文件最后会被正确关闭\n",
    "    with open(file_name, \"w\") as file:\n",
    "        # 遍历列表，写入每一行\n",
    "        for line in files_has_0_distance:\n",
    "            file.write(line + \"\\n\")  # \"\\n\" 是换行符\n",
    "\n",
    "    # print(samples, type(samples))\n",
    "\n",
    "    random.shuffle(samples)\n",
    "    shuffled_samples = samples\n",
    "    sample_collection = shuffled_samples\n",
    "\n",
    "    return sample_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content 1: Parents and students alike may not feel comfortable having their data potentially leaked or shared.\n",
      "Content 2: If AI is able to manage many of the responsibilities of teachers, it may no longer be necessary to pay teachers as much for their work, as they will not be so essential.\n",
      "cosine_similarity:0.1712367832660675\n",
      "Polarity Label: 1\n",
      "------------------------------\n",
      "Content 1: Students have more educational flexibility in VR \\(such as field trips in VR or reality\\) that help students more than school\n",
      "Content 2: VR adds more variety to learning.\n",
      "cosine_similarity:0.7631800770759583\n",
      "Polarity Label: 0\n",
      "------------------------------\n",
      "Content 1: A free kick awarded on this basis could lead to a counterattack.  The shift in formation would be far more dramatic than most free kicks.\n",
      "Content 2: Passes to defenders and to the keeper are rarely intercepted.\n",
      "cosine_similarity:0.39632412791252136\n",
      "Polarity Label: 0\n",
      "------------------------------\n",
      "Content 1: Working in HR is a viable career option for Data Analysts & Data Scientists\n",
      "Content 2: The organisational resources and infrastructure are under-developed and under-utilised, so the requirements of data quality and data access are poorly met for sophisticated and deep analysis, which keeps most analyses on a superficial level and thus boring and repetitive.\n",
      "cosine_similarity:0.3158160448074341\n",
      "Relatedness Label: 1.0\n",
      "------------------------------\n",
      "Content 1: Knowing whether one is in \"command mode\" or \"writing mode\" is haphazard and confusing.\n",
      "Content 2: Powerful language for extending it \\(elisp\\)\n",
      "cosine_similarity:0.03869527205824852\n",
      "Relatedness Label: 0.25\n",
      "------------------------------\n",
      "Content 1: One of those reasons is that it is easier to meet up in VR than in person, where a person might not show up.\n",
      "Content 2: Kids don't retain stuff they hear a teacher say or read in a book the same way, particularly when they are on all types of infobesity-inducing devices these days, such as tv, phones, laptops, games, etc.\n",
      "cosine_similarity:0.0972270518541336\n",
      "Relatedness Label: 0.25\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "polarity_sample_collection = polarity_split_method_1(csv_pair_size_limit, distance_limit)\n",
    "relatedness_sample_collection = relatedness_split_method_1(csv_pair_size_limit, distance_limit)\n",
    "polarity_selected_samples = random.sample(polarity_sample_collection, 3)\n",
    "relatedness_selected_samples = random.sample(relatedness_sample_collection, 3)\n",
    "\n",
    "\n",
    "for idx, sample in enumerate(polarity_selected_samples, 1):\n",
    "    sentence1, sentence2 = sample.texts[0], sample.texts[1]\n",
    "    embeddings = model.encode([sentence1, sentence2], convert_to_tensor=True)\n",
    "\n",
    "    cosine_similarity = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()\n",
    "#     print(f\"Sample {idx}:\")\n",
    "    print(f\"Content 1: {sample.texts[0]}\")\n",
    "    print(f\"Content 2: {sample.texts[1]}\")\n",
    "    print(f\"cosine_similarity:{cosine_similarity}\")\n",
    "    print(f\"Polarity Label: {sample.label}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "for idx, sample in enumerate(relatedness_selected_samples, 1):\n",
    "    sentence1, sentence2 = sample.texts[0], sample.texts[1]\n",
    "    embeddings = model.encode([sentence1, sentence2], convert_to_tensor=True)\n",
    "\n",
    "    cosine_similarity = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()\n",
    "#     print(f\"Sample {idx}:\")\n",
    "    print(f\"Content 1: {sample.texts[0]}\")\n",
    "    print(f\"Content 2: {sample.texts[1]}\")\n",
    "    print(f\"cosine_similarity:{cosine_similarity}\")\n",
    "    print(f\"Relatedness Label: {sample.label}\")\n",
    "    print(\"-\" * 30)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
